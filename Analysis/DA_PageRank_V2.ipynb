{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "#Get the ISO-Codes\n",
    "iso = pd.read_csv(\"../Data/ISO/ISO.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading W_ij Dataframes from CSV files...\n",
      "Loaded ../Data/Networks/W_1_ij.csv as W_1_ij\n",
      "Loaded ../Data/Networks/W_2_ij.csv as W_2_ij\n",
      "Loaded ../Data/Networks/W_3_ij.csv as W_3_ij\n"
     ]
    }
   ],
   "source": [
    "# --- : Load DataFrames from CSV ---\n",
    "print(\"\\nLoading W_ij Dataframes from CSV files...\")\n",
    "W_ij_dataframes = {}\n",
    "csv_filenames = ['../Data/Networks/W_1_ij.csv', '../Data/Networks/W_2_ij.csv', '../Data/Networks/W_3_ij.csv']\n",
    "for filename in csv_filenames:\n",
    "    try:\n",
    "        df_reloaded = pd.read_csv(filename)\n",
    "        base_name = os.path.basename(filename) # Gets 'W_1_ij.csv'\n",
    "        parts = base_name.split('_') # Splits into ['W', '1', 'ij.csv']\n",
    "        \n",
    "        # Check if the parts list has at least 2 elements and the second part is a number\n",
    "        if len(parts) >= 2 and parts[1].isdigit():\n",
    "            df_number = parts[1]\n",
    "            new_df_name = f\"W_{df_number}_ij\"\n",
    "        else:\n",
    "            # Fallback if filename format is unexpected, or use the full name as before\n",
    "            new_df_name = os.path.splitext(base_name)[0] \n",
    "            \n",
    "        W_ij_dataframes[new_df_name] = df_reloaded\n",
    "        print(f\"Loaded {filename} as {new_df_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering DataFrames for periods between 1989 and 2020...\n",
      "Filtered W_1_ij. Original rows: 81353, Filtered rows: 69127\n",
      "Filtered W_2_ij. Original rows: 34585, Filtered rows: 29382\n",
      "Filtered W_3_ij. Original rows: 94775, Filtered rows: 78649\n"
     ]
    }
   ],
   "source": [
    "# Define the desired start and end periods for filtering\n",
    "start_period = 1989 \n",
    "end_period = 2020  \n",
    "\n",
    "print(f\"\\nFiltering DataFrames for periods between {start_period} and {end_period}...\")\n",
    "# Iterate through the dictionary and filter dataframes in-place\n",
    "for df_name, df_content in W_ij_dataframes.items():\n",
    "    if not df_content.empty and 'period' in df_content.columns:\n",
    "        # Apply the period filter directly to the DataFrame in the dictionary\n",
    "        original_rows = len(df_content)\n",
    "        W_ij_dataframes[df_name] = df_content[(df_content['period'] >= start_period) & (df_content['period'] <= end_period)]\n",
    "        print(f\"Filtered {df_name}. Original rows: {original_rows}, Filtered rows: {len(W_ij_dataframes[df_name])}\")\n",
    "    else:\n",
    "        print(f\"Skipping filtering for {df_name}: DataFrame is empty or 'period' column is missing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PageRank for 2006:\n",
      "  Country  PageRank\n",
      "0     DEU  0.150093\n",
      "1     ESP  0.113140\n",
      "2     USA  0.062918\n",
      "3     GBR  0.047150\n",
      "4     NLD  0.041508\n",
      "5     CHN  0.041311\n",
      "6     AUS  0.035041\n",
      "7     ITA  0.033517\n",
      "8     FRA  0.026042\n",
      "9     CAN  0.025946\n"
     ]
    }
   ],
   "source": [
    "# --- 1. PageRank Calculation for a Single Period ---\n",
    "def calculate_pagerank(TN_t_c):\n",
    "    \"\"\"Calculates PageRank for a given period's DataFrame slice.\"\"\"\n",
    "    edges = TN_t_c[['reporterISO', 'partnerISO', 'W_ij']]\n",
    "\n",
    "    countries = sorted(list(set(edges['reporterISO']).union(edges['partnerISO'])))\n",
    "    country_index = {country: i for i, country in enumerate(countries)}\n",
    "    n = len(countries)\n",
    "\n",
    "    if n == 0:\n",
    "        return pd.DataFrame(columns=['Country', 'PageRank']) # Return empty if no nodes\n",
    "\n",
    "    W = np.zeros((n, n))\n",
    "    for index, row in edges.iterrows():\n",
    "        i = country_index[row['reporterISO']]\n",
    "        j = country_index[row['partnerISO']]\n",
    "        W[i, j] += row['W_ij']\n",
    "\n",
    "    row_sums = W.sum(axis=1)\n",
    "    dangling_nodes_mask = (row_sums == 0)\n",
    "\n",
    "    M_row_stochastic = np.divide(W, row_sums[:, np.newaxis], where=row_sums[:, np.newaxis] != 0)\n",
    "    if np.sum(dangling_nodes_mask) > 0:\n",
    "        M_row_stochastic[dangling_nodes_mask, :] = 1.0 / n\n",
    "\n",
    "    P = M_row_stochastic.T # Column-stochastic matrix for PageRank\n",
    "\n",
    "    alpha = 0.85\n",
    "    v = np.ones(n) / n\n",
    "    r = np.ones(n) / n\n",
    "    epsilon = 1e-8\n",
    "    delta = 1.0\n",
    "    iteration = 0\n",
    "    max_iterations = 1000\n",
    "\n",
    "    while delta > epsilon and iteration < max_iterations:\n",
    "        r_new = alpha * P @ r + (1 - alpha) * v\n",
    "        delta = np.linalg.norm(r_new - r, 1)\n",
    "        r = r_new\n",
    "        iteration += 1\n",
    "    \n",
    "    pagerank_t_c = pd.DataFrame({\n",
    "        'Country': countries,\n",
    "        'PageRank': r\n",
    "    }).sort_values(by='PageRank', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return pagerank_t_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Country  PageRank\n",
      "0     CHN  0.084310\n",
      "1     JPN  0.062286\n",
      "2     USA  0.057130\n",
      "3     DEU  0.050889\n",
      "4     KOR  0.033939\n",
      "5     FRA  0.033416\n",
      "6     ITA  0.027790\n",
      "7     SRB  0.025093\n",
      "8     GBR  0.024783\n",
      "9     BEL  0.023188\n"
     ]
    }
   ],
   "source": [
    "def calculate_pagerank_import(TN_t_c):\n",
    "    \"\"\"Calculates Import PageRank for a given period's DataFrame slice.\"\"\"\n",
    "    edges = TN_t_c[['reporterISO', 'partnerISO', 'W_ij']]\n",
    "\n",
    "    countries = sorted(list(set(edges['reporterISO']).union(edges['partnerISO'])))\n",
    "    country_index = {country: i for i, country in enumerate(countries)}\n",
    "    n = len(countries)\n",
    "\n",
    "    if n == 0:\n",
    "        return pd.DataFrame(columns=['Country', 'PageRank']) # Return empty if no nodes\n",
    "\n",
    "    W = np.zeros((n, n))\n",
    "    for index, row in edges.iterrows():\n",
    "        i = country_index[row['reporterISO']]\n",
    "        j = country_index[row['partnerISO']]\n",
    "        W[j, i] += row['W_ij'] # W_ij represents flow from reporterISO (i) to partnerISO (j)\n",
    "\n",
    "    # Calculate row sums of W (sum of exports from each country)\n",
    "    row_sums = W.sum(axis=1)\n",
    "    \n",
    "    # Identify dangling nodes (countries that don't export to anyone)\n",
    "    dangling_nodes_mask = (row_sums == 0)\n",
    "\n",
    "    # M_row_stochastic: M_ij = W_ij / sum(W_ik for k)\n",
    "    # This represents the probability of a link from i to j (i.e., i exports to j)\n",
    "    M_row_stochastic = np.divide(W, row_sums[:, np.newaxis], where=row_sums[:, np.newaxis] != 0)\n",
    "    \n",
    "    # Handle dangling nodes: If a row sums to zero, distribute its \"score\" equally.\n",
    "    # This means a country with no exports distributes its importance to all others.\n",
    "    if np.sum(dangling_nodes_mask) > 0:\n",
    "        M_row_stochastic[dangling_nodes_mask, :] = 1.0 / n\n",
    "\n",
    "    # For IMPORT PageRank, the transition matrix P should be the TRANSPOSE of M_row_stochastic.\n",
    "    # If M_row_stochastic[i,j] is the probability i -> j, then M_row_stochastic.T[i,j] is the probability j -> i.\n",
    "    # This makes P a column-stochastic matrix where P[i,j] represents the probability of moving from j to i.\n",
    "    # The standard PageRank iteration r_new = alpha * P @ r works with a column-stochastic P.\n",
    "    P = M_row_stochastic.T # Column-stochastic matrix for Import PageRank\n",
    "\n",
    "    alpha = 0.85\n",
    "    v = np.ones(n) / n # Personalization vector (uniform distribution)\n",
    "    r = np.ones(n) / n # Initial PageRank vector\n",
    "    epsilon = 1e-8 # Convergence threshold\n",
    "    delta = 1.0 # Current difference between iterations\n",
    "    iteration = 0\n",
    "    max_iterations = 1000 # Maximum number of iterations to prevent infinite loops\n",
    "\n",
    "    while delta > epsilon and iteration < max_iterations:\n",
    "        r_new = alpha * P @ r + (1 - alpha) * v\n",
    "        delta = np.linalg.norm(r_new - r, 1) # L1 norm for convergence check\n",
    "        r = r_new\n",
    "        iteration += 1\n",
    "    \n",
    "    pagerank_t_c = pd.DataFrame({\n",
    "        'Country': countries,\n",
    "        'PageRank': r\n",
    "    }).sort_values(by='PageRank', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return pagerank_t_c\n",
    "\n",
    "# Example usage with your provided variable name:\n",
    "# Assuming TN_t_c is defined somewhere, e.g., it's one of your loaded W_ij_dataframes\n",
    "# For instance:\n",
    "# TN_t_c = W_ij_dataframes\n",
    "# \n",
    "['W_ij_2'] # Using W_ij_2 as an example\n",
    "pagerank_2002_2_imports = calculate_pagerank_import(TN_t_c)\n",
    "print(pagerank_2002_2_imports.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pagerank(df: pd.DataFrame, reporter_col: str, partner_col: str, weight_col: str, damping_factor: float = 0.85, num_iterations: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates PageRank scores for nodes in a graph represented by a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing the graph data.\n",
    "        reporter_col (str): The name of the column representing the source nodes (e.g., 'reporterISO').\n",
    "        partner_col (str): The name of the column representing the target nodes (e.g., 'partnerISO').\n",
    "        weight_col (str): The name of the column representing the edge weights (e.g., 'W_ij').\n",
    "        damping_factor (float, optional): The damping factor for the PageRank algorithm. Defaults to 0.85.\n",
    "        num_iterations (int, optional): The maximum number of iterations for the PageRank calculation. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with 'Node' and 'PageRank' columns, sorted by PageRank in descending order.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure weight column is numeric\n",
    "    df[weight_col] = pd.to_numeric(df[weight_col], errors='coerce')\n",
    "    df.dropna(subset=[weight_col], inplace=True)\n",
    "\n",
    "    # Get all unique nodes\n",
    "    nodes = pd.Series(pd.concat([df[reporter_col], df[partner_col]])).unique()\n",
    "    nodes.sort()\n",
    "    num_nodes = len(nodes)\n",
    "\n",
    "    # Create a mapping from node name to index\n",
    "    node_to_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "    # Create an adjacency matrix\n",
    "    adj_matrix = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        reporter_index = node_to_index[row[reporter_col]]\n",
    "        partner_index = node_to_index[row[partner_col]]\n",
    "        adj_matrix[reporter_index, partner_index] = row[weight_col]\n",
    "\n",
    "    # Normalize the adjacency matrix to create a transition matrix\n",
    "    out_degrees = np.sum(adj_matrix, axis=1)\n",
    "    out_degrees[out_degrees == 0] = 1  # Avoid division by zero for dangling nodes\n",
    "    transition_matrix = adj_matrix / out_degrees[:, np.newaxis]\n",
    "\n",
    "    # Initial PageRank vector (uniform distribution)\n",
    "    pagerank = np.ones(num_nodes) / num_nodes\n",
    "\n",
    "    # Iterative PageRank calculation\n",
    "    for _ in range(num_iterations):\n",
    "        new_pagerank = (1 - damping_factor) / num_nodes + damping_factor * np.dot(transition_matrix.T, pagerank)\n",
    "        if np.allclose(new_pagerank, pagerank):\n",
    "            break\n",
    "        pagerank = new_pagerank\n",
    "\n",
    "    # Create a DataFrame for PageRank scores\n",
    "    pagerank_df = pd.DataFrame({'Node': nodes, 'PageRank': pagerank})\n",
    "    pagerank_df = pagerank_df.sort_values(by='PageRank', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return pagerank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
